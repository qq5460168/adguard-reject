#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
åŠŸèƒ½ï¼šè¯»å– rules.txt ä¸­çš„ QuantumultX åœ¨çº¿è§„åˆ™ URL â†’ æ‰¹é‡æ‹‰å–è§„åˆ™ â†’ åˆå¹¶å¹¶è½¬æ¢ä¸º AdGuard / hosts è§„åˆ™
æ”¹è¿›ç‚¹ï¼š
- æ–°å¢ç™½åå•æ”¯æŒï¼Œå¯è¿‡æ»¤ä¸éœ€è¦çš„è§„åˆ™
- æ›´å¥å£®çš„æ­£åˆ™ï¼Œæ”¯æŒæ›´å®½æ¾çš„åŸŸå/keyword æ•è·ï¼ˆåŒ…æ‹¬ç‚¹å’Œ Unicodeï¼‰
- å»é™¤é‡å¤ï¼Œä¿ç•™æ³¨é‡Šè¡Œå¹¶åœ¨è¾“å‡ºä¸­æ ‡æ³¨æœªè¯†åˆ«è§„åˆ™
- æ›´å‹å¥½çš„é”™è¯¯å¤„ç†ä¸ç»Ÿè®¡ä¿¡æ¯
"""
from typing import List, Set
import re
import requests
import sys
import os

# é…ç½®
URL_CONFIG_FILE = "rules.txt"
ADGUARD_OUTPUT_FILE = "adguard-rules.txt"
WHITE_LIST_FILE = "white.txt"  # æ–°å¢ç™½åå•æ–‡ä»¶
USER_AGENT = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36"
REQUEST_TIMEOUT = 30

# é€šç”¨ç”¨äºæ•è·åŸŸå / å…³é”®è¯çš„æ¨¡å¼ï¼šå°½é‡å®½æ¾ï¼Œæ•è·éé€—å·éç©ºç™½ä¸²ï¼ˆåŒ…æ‹¬å¸¦ç‚¹çš„åŸŸåæˆ–å…³é”®è¯ï¼‰
TOKEN_RE = r'([^\s,]+)'


def read_rule_urls(config_file: str) -> List[str]:
    """è¯»å– rules.txt ä¸­çš„æ‰€æœ‰ URLï¼Œè¿”å›å»é‡åçš„ URL åˆ—è¡¨ï¼ˆå¿½ç•¥ç©ºè¡Œå’Œ # æ³¨é‡Šï¼‰"""
    if not os.path.exists(config_file):
        print(f"âŒ é”™è¯¯ï¼šæœªæ‰¾åˆ°é…ç½®æ–‡ä»¶ {config_file}")
        sys.exit(1)

    with open(config_file, 'r', encoding='utf-8') as f:
        lines = [line.rstrip('\n') for line in f]

    urls = []
    seen = set()
    for line in lines:
        s = line.strip()
        if not s or s.startswith('#'):
            continue
        if s not in seen:
            seen.add(s)
            urls.append(s)

    if not urls:
        print(f"âŒ é”™è¯¯ï¼š{config_file} ä¸­æœªæ‰¾åˆ°ä»»ä½•æœ‰æ•ˆ URL")
        sys.exit(1)

    print(f"ğŸ“‹ ä» {config_file} è¯»å–åˆ° {len(urls)} æ¡è§„åˆ™æºï¼š")
    for i, u in enumerate(urls, 1):
        print(f"  {i}. {u}")
    return urls


def load_white_list() -> Set[str]:
    """åŠ è½½ç™½åå•åŸŸåï¼Œæ”¯æŒ AdGuard ç™½åå•æ ¼å¼ï¼ˆ@@||xxx^ï¼‰å’Œçº¯åŸŸåï¼Œè¿”å›å°å†™åŸŸåé›†åˆ"""
    white_list = set()
    if not os.path.exists(WHITE_LIST_FILE):
        print(f"â„¹ï¸  æœªæ‰¾åˆ°ç™½åå•æ–‡ä»¶ {WHITE_LIST_FILE}ï¼Œå°†ä¸è¿›è¡Œæ’é™¤æ“ä½œ")
        return white_list

    # æ”¯æŒçš„ç™½åå•æ ¼å¼æ­£åˆ™
    adguard_white_pattern = re.compile(r'^@@\||?https?://)?([^|^$]+)')
    domain_pattern = re.compile(r'^([a-zA-Z0-9][a-zA-Z0-9.-]+[a-zA-Z0-9])$')

    try:
        with open(WHITE_LIST_FILE, 'r', encoding='utf-8') as f:
            for line_num, line in enumerate(f, 1):
                line = line.strip()
                if not line or line.startswith('#'):
                    continue  # è·³è¿‡æ³¨é‡Šå’Œç©ºè¡Œ

                # å¤„ç† AdGuard ç™½åå•æ ¼å¼ï¼ˆ@@å¼€å¤´ï¼‰
                adguard_match = adguard_white_pattern.match(line)
                if adguard_match:
                    domain = adguard_match.group(1).strip('.').lower()
                    if domain:
                        white_list.add(domain)
                        continue

                # å¤„ç†çº¯åŸŸåæ ¼å¼
                domain_match = domain_pattern.match(line)
                if domain_match:
                    domain = domain_match.group(1).strip('.').lower()
                    white_list.add(domain)
                    continue

                # æœªè¯†åˆ«çš„æ ¼å¼è­¦å‘Š
                print(f"âš ï¸  ç™½åå•æ–‡ä»¶ç¬¬ {line_num} è¡Œæ ¼å¼ä¸æ”¯æŒï¼š{line}ï¼ˆå·²è·³è¿‡ï¼‰")

        print(f"âœ… åŠ è½½ç™½åå•æˆåŠŸï¼Œå…± {len(white_list)} æ¡æœ‰æ•ˆåŸŸå")
        return white_list
    except OSError as e:
        print(f"âš ï¸  è¯»å–ç™½åå•æ–‡ä»¶å¤±è´¥ï¼š{e}ï¼Œå°†ä¸è¿›è¡Œæ’é™¤æ“ä½œ")
        return set()


def is_whitelisted(rule: str, white_list: Set[str]) -> bool:
    """æ£€æŸ¥è§„åˆ™æ˜¯å¦åœ¨ç™½åå•ä¸­"""
    if not white_list:
        return False

    # æå–è§„åˆ™ä¸­çš„åŸŸåéƒ¨åˆ†
    domain = None
    if rule.startswith('0.0.0.0 '):
        domain = rule.split()[1].lower()
    elif rule.startswith('||') and rule.endswith('^'):
        domain = rule[2:-1].split('/')[0].lower()  # å»æ‰||å’Œ^ï¼Œå¹¶å¿½ç•¥è·¯å¾„éƒ¨åˆ†

    if not domain:
        return False

    # æ£€æŸ¥æ˜¯å¦åŒ¹é…ç™½åå•ï¼ˆæ”¯æŒå­åŸŸåï¼‰
    return any(domain == wl or domain.endswith(f'.{wl}') for wl in white_list)


def fetch_single_url_rules(url: str) -> List[str]:
    """æ‹‰å–å•ä¸ª URL çš„è§„åˆ™æ–‡æœ¬å¹¶æŒ‰è¡Œè¿”å›ï¼ˆè¿‡æ»¤ç©ºè¡Œï¼‰"""
    try:
        print(f"\nğŸ“¥ æ­£åœ¨æ‹‰å–ï¼š{url}")
        r = requests.get(
            url,
            timeout=REQUEST_TIMEOUT,
            headers={"User-Agent": USER_AGENT},
            allow_redirects=True
        )
        r.raise_for_status()
        # å°è¯•å¤šç§ç¼–ç è§£ç ï¼Œæé«˜å…¼å®¹æ€§
        encodings = ['utf-8-sig', 'gbk', 'latin-1']
        text = None
        for encoding in encodings:
            try:
                text = r.content.decode(encoding)
                break
            except UnicodeDecodeError:
                continue
        if text is None:
            raise UnicodeDecodeError("æ— æ³•è§£ç è§„åˆ™å†…å®¹")
            
        lines = [ln.strip() for ln in text.splitlines() if ln.strip()]
        print(f"âœ… æ‹‰å–æˆåŠŸï¼š{len(lines)} è¡Œæœ‰æ•ˆè§„åˆ™")
        return lines
    except requests.exceptions.RequestException as e:
        print(f"âš ï¸  æ‹‰å–å¤±è´¥ï¼š{url} -> {e}")
        return []
    except UnicodeDecodeError:
        print(f"âš ï¸  è§£ç å¤±è´¥ï¼š{url} çš„å†…å®¹æ— æ³•æ­£ç¡®è§£ç ")
        return []


def convert_rule_line(line: str):
    """
    å°†å•è¡Œ QuantumultX è§„åˆ™è½¬æ¢ä¸ºç›®æ ‡è§„åˆ™ã€‚
    è¿”å›ä¸€ä¸ªå­—ç¬¦ä¸²ï¼ˆè½¬æ¢åçš„è§„åˆ™ï¼‰æˆ– Noneï¼ˆæœªè¯†åˆ«æˆ–ä¸éœ€è¦è½¬æ¢ï¼‰ã€‚
    """
    s = line.strip()

    # host,domain,reject  -> hosts
    m = re.match(rf'host\s*,\s*{TOKEN_RE}\s*,\s*reject\s*$', s, re.I)
    if m:
        domain = m.group(1)
        if domain in ('*', ''):
            return None
        return f"0.0.0.0 {domain}"

    # host-suffix,domain, reject -> ||domain^
    m = re.match(rf'host-suffix\s*,\s*{TOKEN_RE}\s*,\s*reject\s*$', s, re.I)
    if m:
        suffix = m.group(1)
        return f"||{suffix}^"

    # host-keyword,keyword, reject -> ||keyword^
    m = re.match(rf'host-keyword\s*,\s*{TOKEN_RE}\s*,\s*reject\s*$', s, re.I)
    if m:
        keyword = m.group(1)
        return f"||{keyword}^"

    # url,protocol://domain/... , reject  -> ||domain/path^
    m = re.match(rf'url\s*,\s*(?:((?:https?|wss?)://)?)([^\s\/,]+)(/[^\s,]*)?\s*,\s*reject\s*$', s, re.I)
    if m:
        domain = m.group(2)
        path = m.group(3) or ""
        return f"||{domain}{path}^"

    # å…¶ä»– reject å½¢å¼ï¼šæš‚ä¸å¤„ç†
    return None


def merge_and_convert(all_rules: List[str], output_file: str, white_list: Set[str]) -> None:
    """åˆå¹¶æ‰€æœ‰è§„åˆ™å¹¶è½¬æ¢å†™å…¥è¾“å‡ºæ–‡ä»¶ï¼Œåº”ç”¨ç™½åå•è¿‡æ»¤"""
    header = [
        "# ===============================",
        "# è‡ªåŠ¨æ‹‰å–+åˆå¹¶+è½¬æ¢è‡ª QuantumultX åœ¨çº¿è§„åˆ™",
        f"# è§„åˆ™æ¥æºé…ç½®ï¼š{URL_CONFIG_FILE}",
        f"# ç™½åå•è¿‡æ»¤ï¼š{len(white_list)} æ¡è§„åˆ™",
        "# æ”¯æŒè§„åˆ™ç±»å‹ï¼ˆå·²å®ç°è½¬æ¢ï¼‰ï¼š",
        "#  - host, åŸŸå, reject  -> hosts: 0.0.0.0 åŸŸå",
        "#  - host-suffix, åŸŸååç¼€, reject -> AdGuard: ||åŸŸå^",
        "#  - host-keyword, å…³é”®è¯, reject -> AdGuard: ||å…³é”®è¯^",
        "#  - url, åè®®://åŸŸå/è·¯å¾„, reject -> AdGuard: ||åŸŸå/è·¯å¾„^",
        "# æ³¨ï¼šä¿ç•™åŸæ³¨é‡Šè¡Œï¼›æœªè¯†åˆ«çš„è§„åˆ™ä¼šä»¥æ³¨é‡Šå½¢å¼å†™å‡ºä»¥ä¾¿äººå·¥æ£€æŸ¥",
        "# ===============================\n"
    ]

    out_lines = []
    out_lines.extend(header)
    converted = set()
    converted_count = 0
    unrecognized_count = 0
    whitelisted_count = 0
    raw_count = len(all_rules)

    for line in all_rules:
        if not line:
            continue
        stripped = line.strip()

        # ä¿ç•™æ³¨é‡Šè¡ŒåŸæ ·ï¼ˆä½†ä¸è®¡å…¥å»é‡ï¼‰
        if stripped.startswith('#'):
            out_lines.append(stripped)
            continue

        # åªå¤„ç†åŒ…å« reject çš„è§„åˆ™
        if 'reject' not in stripped.lower():
            out_lines.append(f"# è·³è¿‡é reject è§„åˆ™ï¼š{stripped}")
            continue

        converted_rule = convert_rule_line(stripped)
        if converted_rule:
            # æ£€æŸ¥æ˜¯å¦åœ¨ç™½åå•ä¸­
            if is_whitelisted(converted_rule, white_list):
                whitelisted_count += 1
                out_lines.append(f"# å·²è¿‡æ»¤ç™½åå•è§„åˆ™ï¼š{converted_rule}")
                continue
                
            if converted_rule not in converted:
                out_lines.append(converted_rule)
                converted.add(converted_rule)
                converted_count += 1
        else:
            out_lines.append(f"# æœªè¯†åˆ«è§„åˆ™ï¼š{stripped}")
            unrecognized_count += 1

    # å†™å…¥æ–‡ä»¶
    try:
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write("\n".join(out_lines))
    except OSError as e:
        print(f"âŒ å†™å…¥æ–‡ä»¶å¤±è´¥ï¼š{output_file} -> {e}")
        sys.exit(1)

    # ç»Ÿè®¡è¾“å‡º
    print("\nâœ… è½¬æ¢å®Œæˆ")
    print(f"  - åŸå§‹è§„åˆ™è¡Œæ•°ï¼š{raw_count}")
    print(f"  - æˆåŠŸè½¬æ¢ï¼ˆå»é‡åï¼‰è§„åˆ™æ•°ï¼š{converted_count}")
    print(f"  - ç™½åå•è¿‡æ»¤è§„åˆ™æ•°ï¼š{whitelisted_count}")
    print(f"  - æœªè¯†åˆ«è§„åˆ™æ•°ï¼ˆå·²å†™ä¸ºæ³¨é‡Šï¼‰ï¼š{unrecognized_count}")
    print(f"  - è¾“å‡ºæ–‡ä»¶ï¼š{output_file}")


def main():
    urls = read_rule_urls(URL_CONFIG_FILE)
    white_list = load_white_list()  # åŠ è½½ç™½åå•
    
    all_rules: List[str] = []
    for url in urls:
        lines = fetch_single_url_rules(url)
        if lines:
            all_rules.extend(lines)
            print(f"  â†’ å½“å‰ç´¯è®¡è§„åˆ™è¡Œæ•°ï¼š{len(all_rules)}")

    if not all_rules:
        print("âŒ é”™è¯¯ï¼šæœªæ‹‰å–åˆ°ä»»ä½•è§„åˆ™ï¼Œé€€å‡ºã€‚")
        sys.exit(1)

    merge_and_convert(all_rules, ADGUARD_OUTPUT_FILE, white_list)


if __name__ == "__main__":
    main()